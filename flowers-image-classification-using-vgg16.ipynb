{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# first of all import application of VGG16 from keras which image size is 224x224\nfrom keras.applications import VGG16\nimage_rows = 224\nimage_cols = 224\n# now load the VGG16 model  from imagenet \nvgg16 = VGG16(weights = 'imagenet',include_top = False, input_shape = (image_rows,\n                                                                      image_cols,3))","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:30:44.817060Z","iopub.execute_input":"2023-03-22T08:30:44.817359Z","iopub.status.idle":"2023-03-22T08:30:58.731207Z","shell.execute_reply.started":"2023-03-22T08:30:44.817329Z","shell.execute_reply":"2023-03-22T08:30:58.730113Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# now let's print our layers\nfor(i,layer) in enumerate (vgg16.layers):\n    print (str(i) + \" \"+ layer.__class__.__name__,layer.trainable)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:05.140022Z","iopub.execute_input":"2023-03-22T08:31:05.140471Z","iopub.status.idle":"2023-03-22T08:31:05.148830Z","shell.execute_reply.started":"2023-03-22T08:31:05.140412Z","shell.execute_reply":"2023-03-22T08:31:05.147573Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"0 InputLayer True\n1 Conv2D True\n2 Conv2D True\n3 MaxPooling2D True\n4 Conv2D True\n5 Conv2D True\n6 MaxPooling2D True\n7 Conv2D True\n8 Conv2D True\n9 Conv2D True\n10 MaxPooling2D True\n11 Conv2D True\n12 Conv2D True\n13 Conv2D True\n14 MaxPooling2D True\n15 Conv2D True\n16 Conv2D True\n17 Conv2D True\n18 MaxPooling2D True\n","output_type":"stream"}]},{"cell_type":"code","source":"# first of all import application of VGG16 from keras which image size is 224x224\nfrom keras.applications import VGG16\nimage_rows = 224\nimage_cols = 224\n# now Reload the VGG16 model  from imagenet  without the top or fully connected layer \nvgg16 = VGG16(weights = 'imagenet',include_top = False, input_shape = (image_rows,\n                                                                      image_cols,3))\n#here we freeze the last four layers\n# layers are set to trainable as True by default \nfor layer in vgg16.layers :\n    layer.trainable = False\n\n#let's print our layers\n# now let's print our layers\nfor(i,layer) in enumerate (vgg16.layers):\n    print (str(i) + \" \"+ layer.__class__.__name__,layer.trainable)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:09.393294Z","iopub.execute_input":"2023-03-22T08:31:09.393989Z","iopub.status.idle":"2023-03-22T08:31:09.719913Z","shell.execute_reply.started":"2023-03-22T08:31:09.393950Z","shell.execute_reply":"2023-03-22T08:31:09.718670Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"0 InputLayer False\n1 Conv2D False\n2 Conv2D False\n3 MaxPooling2D False\n4 Conv2D False\n5 Conv2D False\n6 MaxPooling2D False\n7 Conv2D False\n8 Conv2D False\n9 Conv2D False\n10 MaxPooling2D False\n11 Conv2D False\n12 Conv2D False\n13 Conv2D False\n14 MaxPooling2D False\n15 Conv2D False\n16 Conv2D False\n17 Conv2D False\n18 MaxPooling2D False\n","output_type":"stream"}]},{"cell_type":"code","source":"#create the top or head model that will be  placed ontop of the bottom layers\n\ndef addTopModel(bottom_model,num_classes,D=256):\n    top_model = bottom_model.output \n    top_model = Flatten(name = \"flatten\")(top_model)\n    top_model = Dense(D,activation = \"relu\")(top_model)\n    top_model = Dropout(0.3)(top_model)\n    top_model = Dense(num_classes,activation = \"softmax\")(top_model)\n    return top_model\n   ","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:14.542797Z","iopub.execute_input":"2023-03-22T08:31:14.544212Z","iopub.status.idle":"2023-03-22T08:31:14.552421Z","shell.execute_reply.started":"2023-03-22T08:31:14.544161Z","shell.execute_reply":"2023-03-22T08:31:14.551242Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout , Activation ,Flatten\nfrom keras.layers import Conv2D,MaxPooling2D,ZeroPadding2D\n#from keras.layers.normalization import BatchNormalization\n#from keras.layers.normalization.BatchNormalization\nfrom keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:19.300794Z","iopub.execute_input":"2023-03-22T08:31:19.301165Z","iopub.status.idle":"2023-03-22T08:31:19.307408Z","shell.execute_reply.started":"2023-03-22T08:31:19.301133Z","shell.execute_reply":"2023-03-22T08:31:19.305862Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"num_classes = 17\nFC_Head = addTopModel(vgg16,num_classes)\nmodel = Model(inputs = vgg16.input,outputs = FC_Head)\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:24.043800Z","iopub.execute_input":"2023-03-22T08:31:24.044548Z","iopub.status.idle":"2023-03-22T08:31:24.137483Z","shell.execute_reply.started":"2023-03-22T08:31:24.044484Z","shell.execute_reply":"2023-03-22T08:31:24.136656Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 25088)             0         \n                                                                 \n dense (Dense)               (None, 256)               6422784   \n                                                                 \n dropout (Dropout)           (None, 256)               0         \n                                                                 \n dense_1 (Dense)             (None, 17)                4369      \n                                                                 \n=================================================================\nTotal params: 21,141,841\nTrainable params: 6,427,153\nNon-trainable params: 14,714,688\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator \ntrain_data_dir = '../input/flower-dataset/flowers/train'\nvalidation_data_dir = '../input/flower-dataset/flowers/validation'","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:30.604397Z","iopub.execute_input":"2023-03-22T08:31:30.605099Z","iopub.status.idle":"2023-03-22T08:31:30.644480Z","shell.execute_reply.started":"2023-03-22T08:31:30.605060Z","shell.execute_reply":"2023-03-22T08:31:30.643493Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale = 1/255,rotation_range = 20,\n                                  width_shift_range = 0.2,height_shift_range =0.2,\n                                  horizontal_flip = True,fill_mode ='nearest')","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:39.614570Z","iopub.execute_input":"2023-03-22T08:31:39.614946Z","iopub.status.idle":"2023-03-22T08:31:39.621104Z","shell.execute_reply.started":"2023-03-22T08:31:39.614907Z","shell.execute_reply":"2023-03-22T08:31:39.619800Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"validation_datagen = ImageDataGenerator(rescale = 1/255)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:45.279348Z","iopub.execute_input":"2023-03-22T08:31:45.279840Z","iopub.status.idle":"2023-03-22T08:31:45.286371Z","shell.execute_reply.started":"2023-03-22T08:31:45.279792Z","shell.execute_reply":"2023-03-22T08:31:45.285074Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#change tha batch size according to your system RAM\ntrain_batchsize = 16\nval_batchsize =10","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:48.215890Z","iopub.execute_input":"2023-03-22T08:31:48.217040Z","iopub.status.idle":"2023-03-22T08:31:48.221355Z","shell.execute_reply.started":"2023-03-22T08:31:48.216998Z","shell.execute_reply":"2023-03-22T08:31:48.220169Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(train_data_dir,\n                                                target_size = (image_rows,image_cols))","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:51.492859Z","iopub.execute_input":"2023-03-22T08:31:51.493884Z","iopub.status.idle":"2023-03-22T08:31:51.816050Z","shell.execute_reply.started":"2023-03-22T08:31:51.493829Z","shell.execute_reply":"2023-03-22T08:31:51.814955Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Found 1190 images belonging to 17 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n                                                             target_size = (image_rows,image_cols),\n                                                             batch_size = val_batchsize,\n                                                             class_mode = 'categorical',\n                                                              shuffle = False \n                                                             )","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:31:55.971574Z","iopub.execute_input":"2023-03-22T08:31:55.972299Z","iopub.status.idle":"2023-03-22T08:31:56.083551Z","shell.execute_reply.started":"2023-03-22T08:31:55.972260Z","shell.execute_reply":"2023-03-22T08:31:56.082550Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Found 170 images belonging to 17 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"#training our top layers \nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\ncheckpoint = ModelCheckpoint (\"Trained MOdels/flowers_vgg.h5\",\n                             monitor = \"val_loss\", mode = \"min\",\n                             save_best_only = True,\n                             verbose = 1)\nearlystop = EarlyStopping(monitor = 'val_loss', \n                          min_delta = 0, \n                          patience = 3,\n                          verbose = 1,\n                          restore_best_weights = True)\n\n\n#now use call backs into a call list \ncallbacks = [earlystop ,checkpoint]\n# now compile the model\nmodel.compile(loss = 'categorical_crossentropy',\n             optimizer = RMSprop(lr = 0.001) ,metrics = ['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:32:00.151436Z","iopub.execute_input":"2023-03-22T08:32:00.151817Z","iopub.status.idle":"2023-03-22T08:32:00.173043Z","shell.execute_reply.started":"2023-03-22T08:32:00.151782Z","shell.execute_reply":"2023-03-22T08:32:00.171724Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super().__init__(name, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"nb_train_sample = 1190\nnb_validadtion_sample = 170\nepochs = 3\nbatch_size = 64 \nhistory = model.fit_generator(train_generator,steps_per_epoch=nb_train_sample//batch_size,\n                              epochs = epochs, callbacks= callbacks,\n                              validation_data= validation_generator,\n                              validation_steps= nb_validadtion_sample//batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:32:04.703484Z","iopub.execute_input":"2023-03-22T08:32:04.704182Z","iopub.status.idle":"2023-03-22T08:33:05.806955Z","shell.execute_reply.started":"2023-03-22T08:32:04.704142Z","shell.execute_reply":"2023-03-22T08:33:05.805163Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n18/18 [==============================] - ETA: 0s - loss: 7.3113 - accuracy: 0.1400\nEpoch 1: val_loss improved from inf to 2.36864, saving model to Trained MOdels/flowers_vgg.h5\n18/18 [==============================] - 29s 877ms/step - loss: 7.3113 - accuracy: 0.1400 - val_loss: 2.3686 - val_accuracy: 0.2500\nEpoch 2/3\n18/18 [==============================] - ETA: 0s - loss: 2.2970 - accuracy: 0.2691\nEpoch 2: val_loss did not improve from 2.36864\n18/18 [==============================] - 11s 625ms/step - loss: 2.2970 - accuracy: 0.2691 - val_loss: 2.6000 - val_accuracy: 0.0500\nEpoch 3/3\n18/18 [==============================] - ETA: 0s - loss: 2.2943 - accuracy: 0.3145\nEpoch 3: val_loss improved from 2.36864 to 1.95152, saving model to Trained MOdels/flowers_vgg.h5\n18/18 [==============================] - 11s 595ms/step - loss: 2.2943 - accuracy: 0.3145 - val_loss: 1.9515 - val_accuracy: 0.2500\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"trained models/flowers_vgg.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:46:03.084420Z","iopub.execute_input":"2023-03-22T08:46:03.085145Z","iopub.status.idle":"2023-03-22T08:46:03.253459Z","shell.execute_reply.started":"2023-03-22T08:46:03.085107Z","shell.execute_reply":"2023-03-22T08:46:03.252455Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Now implement  Vgg16 using image size 64x64","metadata":{}},{"cell_type":"code","source":"# first of all import application of VGG16 from keras which image size is 64x64\nfrom keras.applications import VGG16\nimage_rows = 64\nimage_cols = 64\n# now Reload the VGG16 model  from imagenet  without the top or fully connected layer \nvgg16 = VGG16(weights = 'imagenet',include_top = False, input_shape = (image_rows,\n                                                                      image_cols,3))\n#here we freeze the last four layers\n# layers are set to trainable as True by default \nfor layer in vgg16.layers :\n    layer.trainable = False\n\n#let's print our layers\n# now let's print our layers\nfor(i,layer) in enumerate (vgg16.layers):\n    print (str(i) + \" \"+ layer.__class__.__name__,layer.trainable)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:46:12.119995Z","iopub.execute_input":"2023-03-22T08:46:12.120727Z","iopub.status.idle":"2023-03-22T08:46:12.418428Z","shell.execute_reply.started":"2023-03-22T08:46:12.120687Z","shell.execute_reply":"2023-03-22T08:46:12.417201Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"0 InputLayer False\n1 Conv2D False\n2 Conv2D False\n3 MaxPooling2D False\n4 Conv2D False\n5 Conv2D False\n6 MaxPooling2D False\n7 Conv2D False\n8 Conv2D False\n9 Conv2D False\n10 MaxPooling2D False\n11 Conv2D False\n12 Conv2D False\n13 Conv2D False\n14 MaxPooling2D False\n15 Conv2D False\n16 Conv2D False\n17 Conv2D False\n18 MaxPooling2D False\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# create new model using an image size 64x64","metadata":{}},{"cell_type":"code","source":"from keras.applications import VGG16\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n#from keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_data_dir = '../input/flower-dataset/flowers/train'\nvalidation_data_dir = '../input/flower-dataset/flowers/validation'\n\ntrain_datagen = ImageDataGenerator(\n      rescale=1./255,\n      rotation_range=20,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n \nvalidation_datagen = ImageDataGenerator(rescale=1./255)\n \n# Change the batchsize according to your system RAM\ntrain_batchsize = 16\nval_batchsize = 10\n \ntrain_generator = train_datagen.flow_from_directory(\n        train_data_dir,\n        target_size=(image_rows, image_cols),\n        batch_size=train_batchsize,\n        class_mode='categorical')\n \nvalidation_generator = validation_datagen.flow_from_directory(\n        validation_data_dir,\n        target_size=(image_rows, image_cols),\n        batch_size=val_batchsize,\n        class_mode='categorical',\n        shuffle=False)\n\n# Re-loads the VGG16 model without the top or FC layers\nvgg16 = VGG16(weights = 'imagenet', \n                 include_top = False, \n                 input_shape = (image_rows, image_cols, 3))\n\n# Freeze layers\nfor layer in vgg16.layers:\n    layer.trainable = False\n    \n# Number of classes in the Flowers-17 dataset\nnum_classes = 17\n\nFC_Head = addTopModel(vgg16, num_classes)\n\nmodel = Model(inputs=vgg16.input, outputs=FC_Head)\n\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:46:16.576629Z","iopub.execute_input":"2023-03-22T08:46:16.577754Z","iopub.status.idle":"2023-03-22T08:46:17.260633Z","shell.execute_reply.started":"2023-03-22T08:46:16.577711Z","shell.execute_reply":"2023-03-22T08:46:17.259789Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Found 1190 images belonging to 17 classes.\nFound 170 images belonging to 17 classes.\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 64, 64, 3)]       0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 2048)              0         \n                                                                 \n dense_2 (Dense)             (None, 256)               524544    \n                                                                 \n dropout_1 (Dropout)         (None, 256)               0         \n                                                                 \n dense_3 (Dense)             (None, 17)                4369      \n                                                                 \n=================================================================\nTotal params: 15,243,601\nTrainable params: 528,913\nNon-trainable params: 14,714,688\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n                   \ncheckpoint = ModelCheckpoint(\"/home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\",\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             verbose=1)\n\nearlystop = EarlyStopping(monitor = 'val_loss', \n                          min_delta = 0, \n                          patience = 5,\n                          verbose = 1,\n                          restore_best_weights = True)\n\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n                              factor = 0.2,\n                              patience = 3,\n                              verbose = 1,\n                              min_delta = 0.00001)\n\n# we put our call backs into a callback list\ncallbacks = [earlystop, checkpoint, reduce_lr]\n\n# Note we use a very small learning rate \nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = RMSprop(lr = 0.0001),\n              metrics = ['accuracy'])\n\nnb_train_samples = 1190\nnb_validation_samples = 170\nepochs = 25\nbatch_size = 32\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = nb_train_samples // batch_size,\n    epochs = epochs,\n    callbacks = callbacks,\n    validation_data = validation_generator,\n    validation_steps = nb_validation_samples // batch_size)\n\nmodel.save(\"/home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T08:46:38.441297Z","iopub.execute_input":"2023-03-22T08:46:38.442024Z","iopub.status.idle":"2023-03-22T08:48:47.058990Z","shell.execute_reply.started":"2023-03-22T08:46:38.441978Z","shell.execute_reply":"2023-03-22T08:48:47.057823Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","output_type":"stream"},{"name":"stdout","text":"37/37 [==============================] - ETA: 0s - loss: 2.9198 - accuracy: 0.0895\nEpoch 1: val_loss improved from inf to 2.52758, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 7s 161ms/step - loss: 2.9198 - accuracy: 0.0895 - val_loss: 2.5276 - val_accuracy: 0.2600 - lr: 1.0000e-04\nEpoch 2/25\n37/37 [==============================] - ETA: 0s - loss: 2.6498 - accuracy: 0.1639\nEpoch 2: val_loss improved from 2.52758 to 2.45706, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 131ms/step - loss: 2.6498 - accuracy: 0.1639 - val_loss: 2.4571 - val_accuracy: 0.2200 - lr: 1.0000e-04\nEpoch 3/25\n37/37 [==============================] - ETA: 0s - loss: 2.4661 - accuracy: 0.2399\nEpoch 3: val_loss improved from 2.45706 to 2.23144, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 4s 121ms/step - loss: 2.4661 - accuracy: 0.2399 - val_loss: 2.2314 - val_accuracy: 0.3400 - lr: 1.0000e-04\nEpoch 4/25\n37/37 [==============================] - ETA: 0s - loss: 2.2845 - accuracy: 0.3196\nEpoch 4: val_loss improved from 2.23144 to 2.09985, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 137ms/step - loss: 2.2845 - accuracy: 0.3196 - val_loss: 2.0998 - val_accuracy: 0.4000 - lr: 1.0000e-04\nEpoch 5/25\n37/37 [==============================] - ETA: 0s - loss: 2.1429 - accuracy: 0.3378\nEpoch 5: val_loss improved from 2.09985 to 1.99888, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 130ms/step - loss: 2.1429 - accuracy: 0.3378 - val_loss: 1.9989 - val_accuracy: 0.4200 - lr: 1.0000e-04\nEpoch 6/25\n37/37 [==============================] - ETA: 0s - loss: 2.0444 - accuracy: 0.4141\nEpoch 6: val_loss did not improve from 1.99888\n37/37 [==============================] - 5s 133ms/step - loss: 2.0444 - accuracy: 0.4141 - val_loss: 1.9989 - val_accuracy: 0.4000 - lr: 1.0000e-04\nEpoch 7/25\n37/37 [==============================] - ETA: 0s - loss: 1.9826 - accuracy: 0.4003\nEpoch 7: val_loss improved from 1.99888 to 1.91568, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 128ms/step - loss: 1.9826 - accuracy: 0.4003 - val_loss: 1.9157 - val_accuracy: 0.4400 - lr: 1.0000e-04\nEpoch 8/25\n37/37 [==============================] - ETA: 0s - loss: 1.9117 - accuracy: 0.4409\nEpoch 8: val_loss improved from 1.91568 to 1.88951, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 125ms/step - loss: 1.9117 - accuracy: 0.4409 - val_loss: 1.8895 - val_accuracy: 0.3800 - lr: 1.0000e-04\nEpoch 9/25\n37/37 [==============================] - ETA: 0s - loss: 1.8122 - accuracy: 0.4730\nEpoch 9: val_loss improved from 1.88951 to 1.73756, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 127ms/step - loss: 1.8122 - accuracy: 0.4730 - val_loss: 1.7376 - val_accuracy: 0.4600 - lr: 1.0000e-04\nEpoch 10/25\n37/37 [==============================] - ETA: 0s - loss: 1.7095 - accuracy: 0.5223\nEpoch 10: val_loss improved from 1.73756 to 1.67685, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 128ms/step - loss: 1.7095 - accuracy: 0.5223 - val_loss: 1.6768 - val_accuracy: 0.4800 - lr: 1.0000e-04\nEpoch 11/25\n37/37 [==============================] - ETA: 0s - loss: 1.6678 - accuracy: 0.4983\nEpoch 11: val_loss did not improve from 1.67685\n37/37 [==============================] - 4s 111ms/step - loss: 1.6678 - accuracy: 0.4983 - val_loss: 1.7716 - val_accuracy: 0.4000 - lr: 1.0000e-04\nEpoch 12/25\n37/37 [==============================] - ETA: 0s - loss: 1.6586 - accuracy: 0.5270\nEpoch 12: val_loss improved from 1.67685 to 1.64969, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 128ms/step - loss: 1.6586 - accuracy: 0.5270 - val_loss: 1.6497 - val_accuracy: 0.4200 - lr: 1.0000e-04\nEpoch 13/25\n37/37 [==============================] - ETA: 0s - loss: 1.5876 - accuracy: 0.5355\nEpoch 13: val_loss improved from 1.64969 to 1.59442, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 143ms/step - loss: 1.5876 - accuracy: 0.5355 - val_loss: 1.5944 - val_accuracy: 0.4400 - lr: 1.0000e-04\nEpoch 14/25\n37/37 [==============================] - ETA: 0s - loss: 1.5319 - accuracy: 0.5372\nEpoch 14: val_loss did not improve from 1.59442\n37/37 [==============================] - 4s 119ms/step - loss: 1.5319 - accuracy: 0.5372 - val_loss: 1.5948 - val_accuracy: 0.4400 - lr: 1.0000e-04\nEpoch 15/25\n37/37 [==============================] - ETA: 0s - loss: 1.5105 - accuracy: 0.5625\nEpoch 15: val_loss did not improve from 1.59442\n37/37 [==============================] - 5s 126ms/step - loss: 1.5105 - accuracy: 0.5625 - val_loss: 1.6034 - val_accuracy: 0.4400 - lr: 1.0000e-04\nEpoch 16/25\n37/37 [==============================] - ETA: 0s - loss: 1.4143 - accuracy: 0.5911\nEpoch 16: val_loss improved from 1.59442 to 1.55073, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 122ms/step - loss: 1.4143 - accuracy: 0.5911 - val_loss: 1.5507 - val_accuracy: 0.5000 - lr: 1.0000e-04\nEpoch 17/25\n37/37 [==============================] - ETA: 0s - loss: 1.4102 - accuracy: 0.5739\nEpoch 17: val_loss improved from 1.55073 to 1.50386, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 124ms/step - loss: 1.4102 - accuracy: 0.5739 - val_loss: 1.5039 - val_accuracy: 0.4800 - lr: 1.0000e-04\nEpoch 18/25\n37/37 [==============================] - ETA: 0s - loss: 1.3799 - accuracy: 0.5811\nEpoch 18: val_loss did not improve from 1.50386\n37/37 [==============================] - 4s 114ms/step - loss: 1.3799 - accuracy: 0.5811 - val_loss: 1.5120 - val_accuracy: 0.4400 - lr: 1.0000e-04\nEpoch 19/25\n37/37 [==============================] - ETA: 0s - loss: 1.3165 - accuracy: 0.6064\nEpoch 19: val_loss did not improve from 1.50386\n37/37 [==============================] - 5s 137ms/step - loss: 1.3165 - accuracy: 0.6064 - val_loss: 1.5341 - val_accuracy: 0.4400 - lr: 1.0000e-04\nEpoch 20/25\n37/37 [==============================] - ETA: 0s - loss: 1.2879 - accuracy: 0.6203\nEpoch 20: val_loss improved from 1.50386 to 1.44866, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 127ms/step - loss: 1.2879 - accuracy: 0.6203 - val_loss: 1.4487 - val_accuracy: 0.4800 - lr: 1.0000e-04\nEpoch 21/25\n37/37 [==============================] - ETA: 0s - loss: 1.2977 - accuracy: 0.6048\nEpoch 21: val_loss improved from 1.44866 to 1.36149, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 4s 120ms/step - loss: 1.2977 - accuracy: 0.6048 - val_loss: 1.3615 - val_accuracy: 0.5200 - lr: 1.0000e-04\nEpoch 22/25\n37/37 [==============================] - ETA: 0s - loss: 1.2516 - accuracy: 0.6233\nEpoch 22: val_loss did not improve from 1.36149\n37/37 [==============================] - 5s 128ms/step - loss: 1.2516 - accuracy: 0.6233 - val_loss: 1.4634 - val_accuracy: 0.5200 - lr: 1.0000e-04\nEpoch 23/25\n37/37 [==============================] - ETA: 0s - loss: 1.3106 - accuracy: 0.5912\nEpoch 23: val_loss improved from 1.36149 to 1.33839, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 4s 118ms/step - loss: 1.3106 - accuracy: 0.5912 - val_loss: 1.3384 - val_accuracy: 0.5600 - lr: 1.0000e-04\nEpoch 24/25\n37/37 [==============================] - ETA: 0s - loss: 1.1962 - accuracy: 0.6254\nEpoch 24: val_loss did not improve from 1.33839\n37/37 [==============================] - 4s 118ms/step - loss: 1.1962 - accuracy: 0.6254 - val_loss: 1.3740 - val_accuracy: 0.5600 - lr: 1.0000e-04\nEpoch 25/25\n37/37 [==============================] - ETA: 0s - loss: 1.2681 - accuracy: 0.6047\nEpoch 25: val_loss improved from 1.33839 to 1.31302, saving model to /home/deeplearningcv/DeepLearningCV/Trained Models/flowers_vgg_64.h5\n37/37 [==============================] - 5s 129ms/step - loss: 1.2681 - accuracy: 0.6047 - val_loss: 1.3130 - val_accuracy: 0.5200 - lr: 1.0000e-04\n","output_type":"stream"}]}]}